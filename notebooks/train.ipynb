{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q7nlrsOKfAj"
      },
      "source": [
        "## Training guide\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6C8153GMxKA",
        "outputId": "3eff3fbd-ad12-4fa6-daf4-1850fe8098f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/nhtlong/.netrc\n"
          ]
        }
      ],
      "source": [
        "WANDB_TOKEN=None\n",
        "!wandb login --relogin $WANDB_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nhtlong/mambaforge/envs/ucc/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registry of MODEL:\n",
            "╒══════════════════════════╤═══════════════════════════════════════════════════════════════════════════╕\n",
            "│ Names                    │ Objects                                                                   │\n",
            "╞══════════════════════════╪═══════════════════════════════════════════════════════════════════════════╡\n",
            "│ SuperviseModel           │ <class 'core.models.base.SuperviseModel'>                                 │\n",
            "├──────────────────────────┼───────────────────────────────────────────────────────────────────────────┤\n",
            "│ DeeplabV3ResNet50        │ <class 'pkg.models.supervise.deeplabv3.DeeplabV3ResNet50'>                │\n",
            "├──────────────────────────┼───────────────────────────────────────────────────────────────────────────┤\n",
            "│ DeeplabV3ResNet50VicregL │ <class 'pkg.models.supervise.deeplabv3_vicregl.DeeplabV3ResNet50VicregL'> │\n",
            "├──────────────────────────┼───────────────────────────────────────────────────────────────────────────┤\n",
            "│ SegResNet50              │ <class 'pkg.models.supervise.resnet50.SegResNet50'>                       │\n",
            "├──────────────────────────┼───────────────────────────────────────────────────────────────────────────┤\n",
            "│ BackboneWrapper          │ <class 'pkg.models.supervise.resnet50_vicregl.BackboneWrapper'>           │\n",
            "├──────────────────────────┼───────────────────────────────────────────────────────────────────────────┤\n",
            "│ ResNet50VicregL          │ <class 'pkg.models.supervise.resnet50_vicregl.ResNet50VicregL'>           │\n",
            "├──────────────────────────┼───────────────────────────────────────────────────────────────────────────┤\n",
            "│ ResNet50VicregL2         │ <class 'pkg.models.supervise.resnet50_vicregl2.ResNet50VicregL2'>         │\n",
            "├──────────────────────────┼───────────────────────────────────────────────────────────────────────────┤\n",
            "│ SegResNet101             │ <class 'pkg.models.supervise.resnet101.SegResNet101'>                     │\n",
            "├──────────────────────────┼───────────────────────────────────────────────────────────────────────────┤\n",
            "│ DinoV2SFhead             │ <class 'pkg.models.supervise.tf.dinov2.DinoV2SFhead'>                     │\n",
            "├──────────────────────────┼───────────────────────────────────────────────────────────────────────────┤\n",
            "│ DPT                      │ <class 'pkg.models.supervise.tf.dpt.DPT'>                                 │\n",
            "├──────────────────────────┼───────────────────────────────────────────────────────────────────────────┤\n",
            "│ Mask2former              │ <class 'pkg.models.supervise.tf.mask2former.Mask2former'>                 │\n",
            "├──────────────────────────┼───────────────────────────────────────────────────────────────────────────┤\n",
            "│ Segformer                │ <class 'pkg.models.supervise.tf.segformer.Segformer'>                     │\n",
            "├──────────────────────────┼───────────────────────────────────────────────────────────────────────────┤\n",
            "│ EffUnet                  │ <class 'pkg.models.supervise.unet.effunet.EffUnet'>                       │\n",
            "├──────────────────────────┼───────────────────────────────────────────────────────────────────────────┤\n",
            "│ NestedUnet               │ <class 'pkg.models.supervise.unet.nestedunet.NestedUnet'>                 │\n",
            "├──────────────────────────┼───────────────────────────────────────────────────────────────────────────┤\n",
            "│ ResUnet                  │ <class 'pkg.models.supervise.unet.resunet.ResUnet'>                       │\n",
            "├──────────────────────────┼───────────────────────────────────────────────────────────────────────────┤\n",
            "│ Unet                     │ <class 'pkg.models.supervise.unet.unet.Unet'>                             │\n",
            "╘══════════════════════════╧═══════════════════════════════════════════════════════════════════════════╛\n"
          ]
        }
      ],
      "source": [
        "from pkg.models import MODEL_REGISTRY\n",
        "print(MODEL_REGISTRY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding configurating\n",
            "Seed set to 1940\n",
            "Some weights of DPTForSemanticSegmentation were not initialized from the model checkpoint at Intel/dpt-large-ade and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.bias', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.bias', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DPTForSemanticSegmentation were not initialized from the model checkpoint at Intel/dpt-large-ade and are newly initialized because the shapes did not match:\n",
            "- head.head.4.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([2, 256, 1, 1]) in the model instantiated\n",
            "- head.head.4.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "- auxiliary_head.head.4.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([2, 256, 1, 1]) in the model instantiated\n",
            "- auxiliary_head.head.4.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/nhtlong/mambaforge/envs/ucc/lib/python3.8/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/nhtlong/mambaforge/envs/ucc/bin/ucc-train -c . ...\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using find_lr mode, the model will not be trained\n",
            "Missing logger folder: ./runs/dpt-ade-tunelr-2024_01_29-20_36_27/lightning_logs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
            "/home/nhtlong/mambaforge/envs/ucc/lib/python3.8/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "\u001b[1m{\u001b[0m\n",
            "    \u001b[32m'aAcc'\u001b[0m: \u001b[1;36m47.18\u001b[0m,\n",
            "    \u001b[32m'mIoU'\u001b[0m: \u001b[1;36m24.13\u001b[0m,\n",
            "    \u001b[32m'mAcc'\u001b[0m: \u001b[1;36m47.87\u001b[0m,\n",
            "    \u001b[32m'background__IoU'\u001b[0m: \u001b[1;36m46.74\u001b[0m,\n",
            "    \u001b[32m'background__Acc'\u001b[0m: \u001b[1;36m94.14\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__IoU'\u001b[0m: \u001b[1;36m1.52\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__Acc'\u001b[0m: \u001b[1;36m1.61\u001b[0m\n",
            "\u001b[1m}\u001b[0m\n",
            "Finding best initial lr:  99%|████████████████▊| 99/100 [05:36<00:03,  3.40s/it]\n",
            "\u001b[1m{\u001b[0m\n",
            "    \u001b[32m'aAcc'\u001b[0m: \u001b[1;36m50.75\u001b[0m,\n",
            "    \u001b[32m'mIoU'\u001b[0m: \u001b[1;36m25.38\u001b[0m,\n",
            "    \u001b[32m'mAcc'\u001b[0m: \u001b[1;36m50.0\u001b[0m,\n",
            "    \u001b[32m'background__IoU'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
            "    \u001b[32m'background__Acc'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__IoU'\u001b[0m: \u001b[1;36m50.75\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__Acc'\u001b[0m: \u001b[1;36m100.0\u001b[0m\n",
            "\u001b[1m}\u001b[0m\n",
            "LR finder stopped early after 99 steps due to diverging loss.\n",
            "Learning rate set to 0.017378008287493765\n",
            "Restoring states from the checkpoint path at ./runs/dpt-ade-tunelr-2024_01_29-20_36_27/.lr_find_e973912e-d681-4bac-9dec-54572ff5017a.ckpt\n",
            "Restored all states from the checkpoint at ./runs/dpt-ade-tunelr-2024_01_29-20_36_27/.lr_find_e973912e-d681-4bac-9dec-54572ff5017a.ckpt\n",
            "╒═══════════════╤══════════╕\n",
            "│ name          │    value │\n",
            "╞═══════════════╪══════════╡\n",
            "│ learning_rate │ 0.017378 │\n",
            "╘═══════════════╧══════════╛\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=1 ucc-train -c ../configs/public/tf/dpt-ade.yml -o global.find_lr=True global.wandb=False data.args.train.root_dir='../data' data.args.val.root_dir='../data' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding configurating\n",
            "Seed set to 1940\n",
            "/home/nhtlong/mambaforge/envs/ucc2/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/nhtlong/mambaforge/envs/ucc2/bin/ucc-train -c  ...\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnhtlongcs\u001b[0m (\u001b[33mucc-quest-23\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./runs/wandb/run-20231215_205457-xrmx5r53\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbaseline-model-lr-2023_12_15-20_54_56\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ucc-quest-23/ucc-quest-23-public\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ucc-quest-23/ucc-quest-23-public/runs/xrmx5r53\u001b[0m\n",
            "If this is the first time you run this model, you can use global.find_lr=True to find the best lr\n",
            "/home/nhtlong/mambaforge/envs/ucc2/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/nhtlong/mambaforge/envs/ucc2/bin/ucc-train -c  ...\n",
            "[rank: 0] Seed set to 1940\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
            "Overriding configurating\n",
            "[rank: 1] Seed set to 1940\n",
            "If this is the first time you run this model, you can use global.find_lr=True to find the best lr\n",
            "[rank: 1] Seed set to 1940\n",
            "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 2 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "\n",
            "  | Name  | Type             | Params\n",
            "-------------------------------------------\n",
            "0 | model | FCN              | 51.9 M\n",
            "1 | loss  | CrossEntropyLoss | 0     \n",
            "-------------------------------------------\n",
            "51.9 M    Trainable params\n",
            "0         Non-trainable params\n",
            "51.9 M    Total params\n",
            "207.758   Total estimated model params size (MB)\n",
            "Visualizing architecture\u001b[33m...\u001b[0m\n",
            "Cannot log model architecture\n",
            "Visualizing dataset\u001b[33m...\u001b[0m\n",
            "Visualizing architecture\u001b[33m...\u001b[0m\n",
            "Cannot log model architecture\n",
            "Visualizing dataset\u001b[33m...\u001b[0m\n",
            "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]/home/nhtlong/mambaforge/envs/ucc2/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
            "Sanity Checking DataLoader 0:   0%|                      | 0/20 [00:00<?, ?it/s]/home/nhtlong/mambaforge/envs/ucc2/lib/python3.10/site-packages/torch/nn/functional.py:3014: UserWarning: nll_loss2d_forward_out_cuda_template does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352630480/work/aten/src/ATen/Context.cpp:82.)\n",
            "  return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
            "/home/nhtlong/mambaforge/envs/ucc2/lib/python3.10/site-packages/torch/nn/functional.py:3014: UserWarning: nll_loss2d_forward_out_cuda_template does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352630480/work/aten/src/ATen/Context.cpp:82.)\n",
            "  return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
            "/home/nhtlong/mambaforge/envs/ucc2/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "Sanity Checking DataLoader 0: 100%|█████████████| 20/20 [00:08<00:00,  2.33it/s]\u001b[1m{\u001b[0m\n",
            "    \u001b[32m'aAcc'\u001b[0m: \u001b[1;36m53.35\u001b[0m,\n",
            "    \u001b[32m'mIoU'\u001b[0m: \u001b[1;36m30.08\u001b[0m,\n",
            "    \u001b[32m'mAcc'\u001b[0m: \u001b[1;36m52.61\u001b[0m,\n",
            "    \u001b[32m'background__IoU'\u001b[0m: \u001b[1;36m9.1\u001b[0m,\n",
            "    \u001b[32m'background__Acc'\u001b[0m: \u001b[1;36m9.51\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__IoU'\u001b[0m: \u001b[1;36m51.06\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__Acc'\u001b[0m: \u001b[1;36m95.71\u001b[0m\n",
            "\u001b[1m}\u001b[0m\n",
            "\u001b[1m{\u001b[0m\n",
            "    \u001b[32m'aAcc'\u001b[0m: \u001b[1;36m55.3\u001b[0m,\n",
            "    \u001b[32m'mIoU'\u001b[0m: \u001b[1;36m32.05\u001b[0m,\n",
            "    \u001b[32m'mAcc'\u001b[0m: \u001b[1;36m54.31\u001b[0m,\n",
            "    \u001b[32m'background__IoU'\u001b[0m: \u001b[1;36m11.57\u001b[0m,\n",
            "    \u001b[32m'background__Acc'\u001b[0m: \u001b[1;36m11.98\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__IoU'\u001b[0m: \u001b[1;36m52.52\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__Acc'\u001b[0m: \u001b[1;36m96.64\u001b[0m\n",
            "\u001b[1m}\u001b[0m\n",
            "Visualizing model predictions\u001b[33m...\u001b[0m\n",
            "Visualizing model predictions\u001b[33m...\u001b[0m\n",
            "Last batch size is\u001b[33m...\u001b[0m \u001b[1;36m3\u001b[0m\n",
            "Last batch size is\u001b[33m...\u001b[0m \u001b[1;36m3\u001b[0m\n",
            "/home/nhtlong/mambaforge/envs/ucc2/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
            "Epoch 0:   0%|                                          | 0/271 [00:00<?, ?it/s]/home/nhtlong/mambaforge/envs/ucc2/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352630480/work/aten/src/ATen/Context.cpp:82.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "/home/nhtlong/mambaforge/envs/ucc2/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352630480/work/aten/src/ATen/Context.cpp:82.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "Epoch 0: 100%|██| 271/271 [04:53<00:00,  0.92it/s, v_num=5r53, train_loss=0.291]\n",
            "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|                                        | 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|                           | 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   2%|▍                  | 1/43 [00:00<00:15,  2.66it/s]\u001b[A\n",
            "Validation DataLoader 0:   5%|▉                  | 2/43 [00:00<00:14,  2.75it/s]\u001b[A\n",
            "Validation DataLoader 0:   7%|█▎                 | 3/43 [00:01<00:14,  2.78it/s]\u001b[A\n",
            "Validation DataLoader 0:   9%|█▊                 | 4/43 [00:01<00:13,  2.79it/s]\u001b[A\n",
            "Validation DataLoader 0:  12%|██▏                | 5/43 [00:01<00:13,  2.80it/s]\u001b[A\n",
            "Validation DataLoader 0:  14%|██▋                | 6/43 [00:02<00:13,  2.81it/s]\u001b[A\n",
            "Validation DataLoader 0:  16%|███                | 7/43 [00:02<00:12,  2.81it/s]\u001b[A\n",
            "Validation DataLoader 0:  19%|███▌               | 8/43 [00:02<00:12,  2.82it/s]\u001b[A\n",
            "Validation DataLoader 0:  21%|███▉               | 9/43 [00:03<00:12,  2.82it/s]\u001b[A\n",
            "Validation DataLoader 0:  23%|████▏             | 10/43 [00:03<00:11,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  26%|████▌             | 11/43 [00:03<00:11,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  28%|█████             | 12/43 [00:04<00:10,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  30%|█████▍            | 13/43 [00:04<00:10,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  33%|█████▊            | 14/43 [00:04<00:10,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  35%|██████▎           | 15/43 [00:05<00:09,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  37%|██████▋           | 16/43 [00:05<00:09,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  40%|███████           | 17/43 [00:06<00:09,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  42%|███████▌          | 18/43 [00:06<00:08,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  44%|███████▉          | 19/43 [00:06<00:08,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  47%|████████▎         | 20/43 [00:07<00:08,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  49%|████████▊         | 21/43 [00:07<00:07,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  51%|█████████▏        | 22/43 [00:07<00:07,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  53%|█████████▋        | 23/43 [00:08<00:07,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  56%|██████████        | 24/43 [00:08<00:06,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  58%|██████████▍       | 25/43 [00:08<00:06,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  60%|██████████▉       | 26/43 [00:09<00:05,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  63%|███████████▎      | 27/43 [00:09<00:05,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  65%|███████████▋      | 28/43 [00:09<00:05,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|████████████▏     | 29/43 [00:10<00:04,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  70%|████████████▌     | 30/43 [00:10<00:04,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  72%|████████████▉     | 31/43 [00:10<00:04,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  74%|█████████████▍    | 32/43 [00:11<00:03,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  77%|█████████████▊    | 33/43 [00:11<00:03,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  79%|██████████████▏   | 34/43 [00:11<00:03,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  81%|██████████████▋   | 35/43 [00:12<00:02,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|███████████████   | 36/43 [00:12<00:02,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  86%|███████████████▍  | 37/43 [00:13<00:02,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  88%|███████████████▉  | 38/43 [00:13<00:01,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  91%|████████████████▎ | 39/43 [00:13<00:01,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  93%|████████████████▋ | 40/43 [00:14<00:01,  2.84it/s]\u001b[A\n",
            "Validation DataLoader 0:  95%|█████████████████▏| 41/43 [00:14<00:00,  2.85it/s]\u001b[A\n",
            "Validation DataLoader 0:  98%|█████████████████▌| 42/43 [00:14<00:00,  2.85it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████████████| 43/43 [00:15<00:00,  2.85it/s]\u001b[A\u001b[1m{\u001b[0m\n",
            "    \u001b[32m'aAcc'\u001b[0m: \u001b[1;36m86.46\u001b[0m,\n",
            "    \u001b[32m'mIoU'\u001b[0m: \u001b[1;36m76.14\u001b[0m,\n",
            "    \u001b[32m'mAcc'\u001b[0m: \u001b[1;36m86.46\u001b[0m,\n",
            "    \u001b[32m'background__IoU'\u001b[0m: \u001b[1;36m75.59\u001b[0m,\n",
            "    \u001b[32m'background__Acc'\u001b[0m: \u001b[1;36m86.2\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__IoU'\u001b[0m: \u001b[1;36m76.69\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__Acc'\u001b[0m: \u001b[1;36m86.71\u001b[0m\n",
            "\u001b[1m}\u001b[0m\n",
            "\u001b[1m{\u001b[0m\n",
            "    \u001b[32m'aAcc'\u001b[0m: \u001b[1;36m86.22\u001b[0m,\n",
            "    \u001b[32m'mIoU'\u001b[0m: \u001b[1;36m75.77\u001b[0m,\n",
            "    \u001b[32m'mAcc'\u001b[0m: \u001b[1;36m86.21\u001b[0m,\n",
            "    \u001b[32m'background__IoU'\u001b[0m: \u001b[1;36m75.21\u001b[0m,\n",
            "    \u001b[32m'background__Acc'\u001b[0m: \u001b[1;36m84.25\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__IoU'\u001b[0m: \u001b[1;36m76.33\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__Acc'\u001b[0m: \u001b[1;36m88.16\u001b[0m\n",
            "\u001b[1m}\u001b[0m\n",
            "Visualizing model predictions\u001b[33m...\u001b[0m\n",
            "Visualizing model predictions\u001b[33m...\u001b[0m\n",
            "Last batch size is\u001b[33m...\u001b[0m \u001b[1;36m3\u001b[0m\n",
            "Last batch size is\u001b[33m...\u001b[0m \u001b[1;36m3\u001b[0m\n",
            "\n",
            "Epoch 0: 100%|█| 271/271 [05:10<00:00,  0.87it/s, v_num=5r53, train_loss=0.291, Epoch 0, global step 271: 'val_high_vegetation__IoU' reached 76.51000 (best 76.51000), saving model to './runs/ucc-quest-23-public/xrmx5r53/checkpoints/baseline-model-lr-SegResNet101-epoch=0-val_high_vegetation__IoU=76.5100.ckpt' as top 3\n",
            "Epoch 1: 100%|█| 271/271 [04:53<00:00,  0.92it/s, v_num=5r53, train_loss=0.217, \n",
            "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|                                        | 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|                           | 0/43 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   2%|▍                  | 1/43 [00:00<00:16,  2.61it/s]\u001b[A\n",
            "Validation DataLoader 0:   5%|▉                  | 2/43 [00:00<00:15,  2.72it/s]\u001b[A\n",
            "Validation DataLoader 0:   7%|█▎                 | 3/43 [00:01<00:14,  2.75it/s]\u001b[A\n",
            "Validation DataLoader 0:   9%|█▊                 | 4/43 [00:01<00:14,  2.78it/s]\u001b[A\n",
            "Validation DataLoader 0:  12%|██▏                | 5/43 [00:01<00:13,  2.79it/s]\u001b[A\n",
            "Validation DataLoader 0:  14%|██▋                | 6/43 [00:02<00:13,  2.80it/s]\u001b[A\n",
            "Validation DataLoader 0:  16%|███                | 7/43 [00:02<00:12,  2.80it/s]\u001b[A\n",
            "Validation DataLoader 0:  19%|███▌               | 8/43 [00:02<00:12,  2.81it/s]\u001b[A\n",
            "Validation DataLoader 0:  21%|███▉               | 9/43 [00:03<00:12,  2.81it/s]\u001b[A\n",
            "Validation DataLoader 0:  23%|████▏             | 10/43 [00:03<00:11,  2.82it/s]\u001b[A\n",
            "Validation DataLoader 0:  26%|████▌             | 11/43 [00:03<00:11,  2.82it/s]\u001b[A\n",
            "Validation DataLoader 0:  28%|█████             | 12/43 [00:04<00:10,  2.82it/s]\u001b[A\n",
            "Validation DataLoader 0:  30%|█████▍            | 13/43 [00:04<00:10,  2.82it/s]\u001b[A\n",
            "Validation DataLoader 0:  33%|█████▊            | 14/43 [00:04<00:10,  2.82it/s]\u001b[A\n",
            "Validation DataLoader 0:  35%|██████▎           | 15/43 [00:05<00:09,  2.82it/s]\u001b[A\n",
            "Validation DataLoader 0:  37%|██████▋           | 16/43 [00:05<00:09,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  40%|███████           | 17/43 [00:06<00:09,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  42%|███████▌          | 18/43 [00:06<00:08,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  44%|███████▉          | 19/43 [00:06<00:08,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  47%|████████▎         | 20/43 [00:07<00:08,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  49%|████████▊         | 21/43 [00:07<00:07,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  51%|█████████▏        | 22/43 [00:07<00:07,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  53%|█████████▋        | 23/43 [00:08<00:07,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  56%|██████████        | 24/43 [00:08<00:06,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  58%|██████████▍       | 25/43 [00:08<00:06,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  60%|██████████▉       | 26/43 [00:09<00:06,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  63%|███████████▎      | 27/43 [00:09<00:05,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  65%|███████████▋      | 28/43 [00:09<00:05,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|████████████▏     | 29/43 [00:10<00:04,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  70%|████████████▌     | 30/43 [00:10<00:04,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  72%|████████████▉     | 31/43 [00:10<00:04,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  74%|█████████████▍    | 32/43 [00:11<00:03,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  77%|█████████████▊    | 33/43 [00:11<00:03,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  79%|██████████████▏   | 34/43 [00:11<00:03,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  81%|██████████████▋   | 35/43 [00:12<00:02,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  84%|███████████████   | 36/43 [00:12<00:02,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  86%|███████████████▍  | 37/43 [00:13<00:02,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  88%|███████████████▉  | 38/43 [00:13<00:01,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  91%|████████████████▎ | 39/43 [00:13<00:01,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  93%|████████████████▋ | 40/43 [00:14<00:01,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  95%|█████████████████▏| 41/43 [00:14<00:00,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  98%|█████████████████▌| 42/43 [00:14<00:00,  2.83it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████████████| 43/43 [00:15<00:00,  2.83it/s]\u001b[A\u001b[1m{\u001b[0m\n",
            "    \u001b[32m'aAcc'\u001b[0m: \u001b[1;36m88.55\u001b[0m,\n",
            "    \u001b[32m'mIoU'\u001b[0m: \u001b[1;36m79.42\u001b[0m,\n",
            "    \u001b[32m'mAcc'\u001b[0m: \u001b[1;36m88.52\u001b[0m,\n",
            "    \u001b[32m'background__IoU'\u001b[0m: \u001b[1;36m78.7\u001b[0m,\n",
            "    \u001b[32m'background__Acc'\u001b[0m: \u001b[1;36m85.31\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__IoU'\u001b[0m: \u001b[1;36m80.14\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__Acc'\u001b[0m: \u001b[1;36m91.73\u001b[0m\n",
            "\u001b[1m}\u001b[0m\n",
            "\u001b[1m{\u001b[0m\n",
            "    \u001b[32m'aAcc'\u001b[0m: \u001b[1;36m88.76\u001b[0m,\n",
            "    \u001b[32m'mIoU'\u001b[0m: \u001b[1;36m79.75\u001b[0m,\n",
            "    \u001b[32m'mAcc'\u001b[0m: \u001b[1;36m88.7\u001b[0m,\n",
            "    \u001b[32m'background__IoU'\u001b[0m: \u001b[1;36m78.92\u001b[0m,\n",
            "    \u001b[32m'background__Acc'\u001b[0m: \u001b[1;36m86.57\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__IoU'\u001b[0m: \u001b[1;36m80.58\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__Acc'\u001b[0m: \u001b[1;36m90.82\u001b[0m\n",
            "\u001b[1m}\u001b[0m\n",
            "Visualizing model predictions\u001b[33m...\u001b[0m\n",
            "Visualizing model predictions\u001b[33m...\u001b[0m\n",
            "Last batch size is\u001b[33m...\u001b[0m \u001b[1;36m3\u001b[0m\n",
            "Last batch size is\u001b[33m...\u001b[0m \u001b[1;36m3\u001b[0m\n",
            "\n",
            "Epoch 1: 100%|█| 271/271 [05:10<00:00,  0.87it/s, v_num=5r53, train_loss=0.217, Epoch 1, global step 542: 'val_high_vegetation__IoU' reached 80.36000 (best 80.36000), saving model to './runs/ucc-quest-23-public/xrmx5r53/checkpoints/baseline-model-lr-SegResNet101-epoch=1-val_high_vegetation__IoU=80.3600.ckpt' as top 3\n",
            "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
            "Epoch 1: 100%|█| 271/271 [05:10<00:00,  0.87it/s, v_num=5r53, train_loss=0.217, \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   lr-SGD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss █▆▄▃▃▄▂▄▃▄▂▂▂▃▂▂▃▂▃▂▁▂▁▃▂▃▂▃▂▂▂▂▂▂▂▁▂▃▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_aAcc ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_background__Acc ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_background__IoU ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: val_high_vegetation__Acc ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: val_high_vegetation__IoU ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_loss █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_mAcc ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_mIoU ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    epoch 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   lr-SGD 0.036\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss 0.19396\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/global_step 542\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_aAcc 88.655\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_background__Acc 85.94\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_background__IoU 78.81\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: val_high_vegetation__Acc 91.275\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: val_high_vegetation__IoU 80.36\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_loss 0.2629\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_mAcc 88.61\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_mIoU 79.585\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbaseline-model-lr-2023_12_15-20_54_56\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ucc-quest-23/ucc-quest-23-public/runs/xrmx5r53\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 5 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./runs/wandb/run-20231215_205457-xrmx5r53/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1 ucc-train -c  ../configs/public/unet/base.yml \\\n",
        "                                    -o  global.find_lr=False \\\n",
        "                                        global.wandb=True \\\n",
        "                                        data.args.train.root_dir='../data' \\\n",
        "                                        data.args.val.root_dir='../data' \\\n",
        "                                        trainer.learning_rate=0.036 \\\n",
        "                                        trainer.num_epochs=2 #notebook only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/home/nhtlong/DCU/ucc-seg/segmentation/runs/ucc-quest-23-public-lb/3tjf1aku/checkpoints/best.ckpt',\n",
              " 'dpt-ade-DPT-epoch=24-val_high_vegetation__IoU=84.2900.ckpt')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os \n",
        "import os.path as osp\n",
        "import shutil \n",
        "\n",
        "RUNS_FOLDER='/home/nhtlong/DCU/ucc-seg/segmentation/runs/'\n",
        "PROJECT_NAME='ucc-quest-23-public-lb'\n",
        "RUN_ID='3tjf1aku'\n",
        "CKPT_DIR=osp.join(RUNS_FOLDER, PROJECT_NAME, RUN_ID, 'checkpoints')\n",
        "CKPT_PATH=osp.join(RUNS_FOLDER, PROJECT_NAME, RUN_ID, 'checkpoints', 'best.ckpt')\n",
        "\n",
        "best_ckpt = os.listdir(CKPT_DIR)\n",
        "best_ckpt.sort()\n",
        "best_ckpt = best_ckpt[-1]\n",
        "shutil.copy(osp.join(CKPT_DIR, best_ckpt), CKPT_PATH)\n",
        "CKPT_PATH, best_ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding configurating\n",
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b3-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([2, 768, 1, 1]) in the model instantiated\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-ade-640-640 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([2, 768, 1, 1]) in the model instantiated\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Validating: 100%|█████████████████████████████| 689/689 [00:56<00:00, 12.26it/s]\n",
            "\u001b[1m{\u001b[0m\n",
            "    \u001b[32m'aAcc'\u001b[0m: \u001b[1;36m91.24\u001b[0m,\n",
            "    \u001b[32m'mIoU'\u001b[0m: \u001b[1;36m83.89\u001b[0m,\n",
            "    \u001b[32m'mAcc'\u001b[0m: \u001b[1;36m91.22\u001b[0m,\n",
            "    \u001b[32m'background__IoU'\u001b[0m: \u001b[1;36m83.45\u001b[0m,\n",
            "    \u001b[32m'background__Acc'\u001b[0m: \u001b[1;36m89.31\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__IoU'\u001b[0m: \u001b[1;36m84.32\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__Acc'\u001b[0m: \u001b[1;36m93.13\u001b[0m\n",
            "\u001b[1m}\u001b[0m\n",
            "CPU times: user 1.65 s, sys: 180 ms, total: 1.83 s\n",
            "Wall time: 1min 18s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# EVAL WITH ORIGINAL SIZE\n",
        "!ucc-eval   -c  ../configs/public/tf/segformerb5-ade.yml \\\n",
        "            -o  global.pretrained=$CKPT_PATH \\\n",
        "                data.args.train.root_dir='../data' \\\n",
        "                data.args.val.root_dir='../data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding configurating\n",
            "Some weights of DPTForSemanticSegmentation were not initialized from the model checkpoint at Intel/dpt-large-ade and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.bias', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.bias', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DPTForSemanticSegmentation were not initialized from the model checkpoint at Intel/dpt-large-ade and are newly initialized because the shapes did not match:\n",
            "- head.head.4.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([2, 256, 1, 1]) in the model instantiated\n",
            "- head.head.4.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "- auxiliary_head.head.4.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([2, 256, 1, 1]) in the model instantiated\n",
            "- auxiliary_head.head.4.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/nhtlong/mambaforge/envs/ucc/lib/python3.8/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/nhtlong/mambaforge/envs/ucc/bin/ucc-eval-fast  ...\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "/home/nhtlong/.local/lib/python3.8/site-packages/albumentations/augmentations/blur/transforms.py:184: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
            "  warnings.warn(\n",
            "/home/nhtlong/.local/lib/python3.8/site-packages/albumentations/augmentations/transforms.py:1554: UserWarning: Using default interpolation INTER_NEAREST, which is sub-optimal.Please specify interpolation mode for downscale and upscale explicitly.For additional information see this PR https://github.com/albumentations-team/albumentations/pull/584\n",
            "  warnings.warn(\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "Validation DataLoader 0:   0%|                          | 0/172 [00:00<?, ?it/s]/home/nhtlong/mambaforge/envs/ucc/lib/python3.8/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "Validation DataLoader 0: 100%|████████████████| 172/172 [00:57<00:00,  3.01it/s]\u001b[1m{\u001b[0m\n",
            "    \u001b[32m'aAcc'\u001b[0m: \u001b[1;36m87.04\u001b[0m,\n",
            "    \u001b[32m'mIoU'\u001b[0m: \u001b[1;36m76.9\u001b[0m,\n",
            "    \u001b[32m'mAcc'\u001b[0m: \u001b[1;36m86.9\u001b[0m,\n",
            "    \u001b[32m'background__IoU'\u001b[0m: \u001b[1;36m74.98\u001b[0m,\n",
            "    \u001b[32m'background__Acc'\u001b[0m: \u001b[1;36m79.05\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__IoU'\u001b[0m: \u001b[1;36m78.81\u001b[0m,\n",
            "    \u001b[32m'high_vegetation__Acc'\u001b[0m: \u001b[1;36m94.75\u001b[0m\n",
            "\u001b[1m}\u001b[0m\n",
            "Validation DataLoader 0: 100%|████████████████| 172/172 [00:57<00:00,  3.00it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m        val_aAcc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          87.04          \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m   val_background__Acc   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          79.05          \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m   val_background__IoU   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          74.98          \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36mval_high_vegetation__Acc \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          94.75          \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36mval_high_vegetation__IoU \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          78.81          \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5112203359603882    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        val_mAcc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          86.9           \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        val_mIoU         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          76.9           \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n"
          ]
        }
      ],
      "source": [
        "# EVAL WITH RESIZED IMGS\n",
        "!ucc-eval-fast  -c  ../configs/public/tf/dpt-ade.yml \\\n",
        "                -o  global.pretrained=$CKPT_PATH \\\n",
        "                    data.args.train.root_dir='../data' \\\n",
        "                    data.args.val.root_dir='../data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p ./tmp/valid/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    username: ucc-quest-23\n",
            "    project_name: \"ucc-quest-23-public-lb\"\n",
            "    name: \"dpt-ade-tunelr\" # experiment name\n",
            "    name: SegDataset\n",
            "model:\n",
            "    name: DPT\n",
            "        NUM_CLASS: 2\n",
            "        PRETRAINED: null\n",
            "    - name: SMAPIoUMetricWrapper\n",
            "    - name: ModelCheckpoint\n",
            "          filename: \"{epoch}-{val_high_vegetation__IoU:.4f}\"\n",
            "    - name: EarlyStopping\n",
            "    - name: LearningRateMonitor\n",
            "    - name: SemanticVisualizerCallbackWanDB\n"
          ]
        }
      ],
      "source": [
        "!cat ../configs/public/tf/dpt-ade.yml | grep -E \"model|name|PRETRAINED|NUM_CLASS\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./tmp/valid/inference.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./tmp/valid/inference.yml\n",
        "global:\n",
        "    pretrained: null\n",
        "    save_path: null\n",
        "    batch_size: 1\n",
        "data:\n",
        "    SIZE: 380 # ORIGINAL SIZE\n",
        "    IMG_DIR: null\n",
        "model:\n",
        "    name: DPT\n",
        "    args:\n",
        "        NUM_CLASS: 2\n",
        "        PRETRAINED: null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding configurating\n",
            "Some weights of DPTForSemanticSegmentation were not initialized from the model checkpoint at Intel/dpt-large-ade and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_var', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.bias', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DPTForSemanticSegmentation were not initialized from the model checkpoint at Intel/dpt-large-ade and are newly initialized because the shapes did not match:\n",
            "- head.head.4.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([2, 256, 1, 1]) in the model instantiated\n",
            "- head.head.4.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "- auxiliary_head.head.4.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([2, 256, 1, 1]) in the model instantiated\n",
            "- auxiliary_head.head.4.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[WARNING] learning rate is not defined, auto set to 1e-3\n",
            "/home/nhtlong/mambaforge/envs/ucc/lib/python3.8/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/nhtlong/mambaforge/envs/ucc/bin/ucc-pred -c ./ ...\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "Predicting DataLoader 0: 100%|████████████████| 689/689 [00:54<00:00, 12.57it/s]\n"
          ]
        }
      ],
      "source": [
        "!ucc-pred   -c  ./tmp/valid/inference.yml \\\n",
        "            -o  global.pretrained=$CKPT_PATH \\\n",
        "                data.IMG_DIR=../data/public/img/valid/ \\\n",
        "                global.save_path=./tmp/valid/results.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "[AIC22][VER] Training Colors Classifier guide.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('zaloai')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "20ae68c5166a594adc0b9bc0156f781891935baaf9f86f15a7c294214ff40d3d"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
